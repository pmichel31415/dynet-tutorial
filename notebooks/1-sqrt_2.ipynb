{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynet tutorial: Approximating $\\sqrt{2}$ with gradient descent\n",
    "\n",
    "Here's a quick tutorial to get to know the dynet workflow on a simple optimization problem: computing the value of $\\sqrt{2}$.\n",
    "\n",
    "This can be reframed as an optimization problem where we are looking for \n",
    "\n",
    "$$\\text{argmin}_{0\\leq \\theta\\leq 2} \\left[\\theta^2-2\\right]^2$$\n",
    "\n",
    "which we can solve via gradient descent. Note that this function is *not* convex on this domain (its second derivative, $4(3\\theta^2-2)$, is negative near zero for instance). However it does have only one global minimum on the $[0, 2]$ interval.\n",
    "\n",
    "Here's how to solve this problem in dynet:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dynet and numpy to get started\n",
    "\n",
    "I'm assuming that you're using the latest version of dynet (I wrote the tutorial on [this commit](https://github.com/clab/dynet/tree/d060d1103424cfecfaa39739d21d8e37818170be) but the latest master should do)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dynet as dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will define the parameters of our optimization problems. Parameters in dynet are stored in a `ParameterCollection`. The `ParameterCollection` is used to save/load and update multiple parameters a the same time. Here we only have one (scalar) parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial value: theta=0.3572946191\n"
     ]
    }
   ],
   "source": [
    "# Declare the parameter collection\n",
    "pc = dy.ParameterCollection()\n",
    "# Add our parameter to the collection. Here the shape of the parameter is just 1: it's a scalar.\n",
    "# The parameter will be initialized at random between 0 and 2 (we know our solution to be in this interval)\n",
    "theta = pc.add_parameters(1, init=np.random.uniform(0, 2))\n",
    "# Let's print the value\n",
    "# First retrieve the parmeter value as a numpy array of shape (1,)\n",
    "theta_val = theta.as_array()\n",
    "# Then print it!\n",
    "print('Initial value: theta=%.10f' % theta_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will declare our `Trainer` object which will define the optimization method that we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations\n",
    "n_iterations=20\n",
    "# Learning rate\n",
    "learning_rate=0.1\n",
    "# SGD trainer (ie vanilla gradient descent)\n",
    "trainer = dy.SimpleSGDTrainer(pc, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start optimizing our objective. The optimization cycle always follows the same order in dynet:\n",
    "1. Call `dy.renew_cg()`: this basically clears all previous computation. This is VERY important, think of it as garbage collection.\n",
    "2. Add the constants to the computation graph (in the case of neural nets that would be the inputs).\n",
    "3. Build the objective function with dynet's built-in operations: no computation is performed yet, but dynet will build a computation graph with all the operations in the right order\n",
    "4. Call the forward pass on the objective: this will compute the value of the objective function and all intermediate values needed for the backward pass\n",
    "5. Call the backward pass on the objective: this is where the magic happens! Every operation has its derivative computed for you, and using the chain rule dynet automatically derives the gradient of the objective with respect ot the parameters\n",
    "6. Call `.update()` on your trainer object to update the parameters according to your optimizer\n",
    "7. Back to step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss @ 1 iterations:\t3.5056591034\n",
      "Loss @ 2 iterations:\t2.5905482769\n",
      "Loss @ 3 iterations:\t0.8927946091\n",
      "Loss @ 4 iterations:\t0.0000116434\n",
      "Loss @ 5 iterations:\t0.0000042144\n",
      "Loss @ 6 iterations:\t0.0000015123\n",
      "Loss @ 7 iterations:\t0.0000005452\n",
      "Loss @ 8 iterations:\t0.0000001960\n",
      "Loss @ 9 iterations:\t0.0000000706\n",
      "Loss @ 10 iterations:\t0.0000000254\n",
      "Loss @ 11 iterations:\t0.0000000092\n",
      "Loss @ 12 iterations:\t0.0000000033\n",
      "Loss @ 13 iterations:\t0.0000000012\n",
      "Loss @ 14 iterations:\t0.0000000004\n",
      "Loss @ 15 iterations:\t0.0000000001\n",
      "Loss @ 16 iterations:\t0.0000000001\n",
      "Loss @ 17 iterations:\t0.0000000000\n",
      "Loss @ 18 iterations:\t0.0000000000\n",
      "Loss @ 19 iterations:\t0.0000000000\n",
      "Loss @ 20 iterations:\t0.0000000000\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(n_iterations):\n",
    "    # Step 1: renew the computation graph\n",
    "    dy.renew_cg()\n",
    "    # Step 2: add constants in the computation graph\n",
    "    two = dy.scalarInput(2)   # Input the scalar 2\n",
    "    # Step 3: define the loss function: here the loss is (theta^2 - 2)^2\n",
    "    loss = dy.square(dy.square(theta) - two)\n",
    "    # Step 4: Call the forward pass to compute the value of the loss function (and then print it)\n",
    "    loss.forward()\n",
    "    loss_value = loss.value() # Retrieve the value from the computation graph\n",
    "    print('Loss @ %d iterations:\\t%.10f' % (iteration+1, loss_value))\n",
    "    # Step 5: Call the backward pass (this computes the gradients)\n",
    "    loss.backward()\n",
    "    # Step 6: update the parameter(s)\n",
    "    trainer.update()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then look at the final values for $\\theta$ and $\\theta^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final values:\n",
      "\ttheta\t= 1.4142132998\n",
      "\ttheta^2\t= 1.9999992572\n"
     ]
    }
   ],
   "source": [
    "theta_val = theta.as_array()\n",
    "print('Final values:\\n\\ttheta\\t= %.10f\\n\\ttheta^2\\t= %.10f' % (theta_val, theta_val**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the difference with numpy's value for $\\sqrt{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance to sqrt(2): 2.6e-07\n"
     ]
    }
   ],
   "source": [
    "target = np.sqrt(2)\n",
    "print('Distance to sqrt(2): %.1e' % np.abs(target-theta_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Not too shabby!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
